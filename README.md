# SpeechLabs

**Creating confidence through user-driven feedback.**  
Advanced speech analysis powered by AI - at no cost.  
Perfect for public speaking, presentations, interviews, or confidence-building exercises.

---

## Overview

**SpeechLabs** is an AI-powered speech analysis tool that evaluates videos of users speaking. It analyzes **speech emotion** and **delivery patterns**, then generates personalized feedback using Google Gemini AI and provides transcription through Deepgram's Speech-to-Text API. The goal: help users overcome social anxiety, improve delivery, and speak with confidence.

---

## Key Features

- ğŸ¥ **Video Upload** â€“ Users upload a video of themselves speaking
- ğŸ”Š **Speech Emotion Recognition** â€“ Detects tone, mood, and speaking style using pre-trained models
- ğŸ“Š **Speaking Rate Analysis** â€“ Measures words per second and provides visual feedback
- ğŸ§  **AI-Powered Feedback** â€“ LLM-generated insights and tips to improve delivery using Google Gemini
- ğŸ’¬ **AI Speech Coach** â€“ Interactive chatbot for personalized advice based on your speech patterns
- ğŸ”Š **Text-to-Speech Feedback** â€“ Deepgram TTS reads coaching advice aloud
- ğŸ“ˆ **Interactive Visualizations** â€“ View detailed timelines of emotion patterns and speaking rate

---

## Tech Stack

### Backend
- **Flask** â€“ REST API backend
- **Python 3.8+** â€“ Core logic
- **Hugging Face Transformers** â€“ Speech emotion recognition (Wav2Vec2)
- **Deepgram API** â€“ Speech-to-text transcription with Smart Formatting
- **Deepgram TTS** â€“ Text-to-speech for audio feedback
- **Google Gemini API** â€“ AI feedback generation
- **FFmpeg** â€“ Audio extraction and processing

### Frontend
- **React** â€“ User interface
- **React Router** â€“ Client-side routing
- **Recharts** â€“ Data visualization
- **CSS Modules** â€“ Component styling

---

## Getting Started

### Prerequisites
- Python 3.8+
- Node.js 14+
- FFmpeg installed on your system
- Google Gemini API key
- Deepgram API key

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/speechlabs.git
   cd speechlabs
   ```

2. **Set up the backend**
   ```bash
   cd backend
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   pip install -r requirements.txt
   ```

3. **Create a .env file in the backend directory**
   ```env
   GEMINI_API_KEY=your_gemini_api_key_here
   DEEPGRAM_API_KEY=your_deepgram_api_key_here
   FFMPEG_PATH=ffmpeg  # Only if FFmpeg is not in your PATH
   ```

4. **Set up the frontend**
   ```bash
   cd ../frontend
   npm install
   ```

### Running the Application

1. **Start the Flask backend**
   ```bash
   cd backend
   python app.py
   ```
   The backend will run on `http://localhost:5000`

2. **Start the React frontend** (in a new terminal)
   ```bash
   cd frontend
   npm start
   ```
   The frontend will run on `http://localhost:3000`

3. **Open your browser and go to http://localhost:3000**

---

## Project Structure

```
speechlabs/
â”œâ”€â”€ backend/                   # Flask backend
â”‚   â”œâ”€â”€ api/                   # API routes
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ routes.py          # Upload, chat, and health check endpoints
â”‚   â”œâ”€â”€ services/              # Core services
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ audio_service.py   # Audio segmentation with FFmpeg
â”‚   â”‚   â”œâ”€â”€ deepgram_service.py # Deepgram STT and TTS
â”‚   â”‚   â”œâ”€â”€ gemini_service.py  # Gemini AI feedback generation
â”‚   â”‚   â””â”€â”€ speech_analysis.py # Emotion recognition
â”‚   â”œâ”€â”€ utils/                 # Utilities
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_processor.py  # Data processing and formatting
â”‚   â”‚   â””â”€â”€ visualization.py   # Visualization data preparation
â”‚   â”œâ”€â”€ app.py                 # Main Flask application
â”‚   â”œâ”€â”€ requirements.txt       # Backend dependencies
â”‚   â””â”€â”€ .env.example           # Environment variables template
â”‚
â”œâ”€â”€ frontend/                  # React frontend
â”‚   â”œâ”€â”€ public/                # Static files
â”‚   â”‚   â”œâ”€â”€ index.html
â”‚   â”‚   â”œâ”€â”€ manifest.json
â”‚   â”‚   â””â”€â”€ favicon.ico
â”‚   â”œâ”€â”€ src/                   # Source code
â”‚   â”‚   â”œâ”€â”€ components/        # Reusable UI components
â”‚   â”‚   â”‚   â”œâ”€â”€ layout/        # Layout components
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Card.js
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Footer.js
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Header.js
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Loading.js
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ TabPanel.js
â”‚   â”‚   â”‚   â”œâ”€â”€ CoachChat.js   # AI coach chatbot
â”‚   â”‚   â”‚   â”œâ”€â”€ EmotionTimeline.js # Emotion visualization
â”‚   â”‚   â”‚   â”œâ”€â”€ FeatureList.js # Feature cards
â”‚   â”‚   â”‚   â”œâ”€â”€ InsightPanel.js # Analysis insights
â”‚   â”‚   â”‚   â”œâ”€â”€ TranscriptView.js # Transcript display
â”‚   â”‚   â”‚   â””â”€â”€ VideoUploader.js # Video upload interface
â”‚   â”‚   â”œâ”€â”€ pages/             # Application pages
â”‚   â”‚   â”‚   â”œâ”€â”€ Analysis.js    # Analysis results page
â”‚   â”‚   â”‚   â”œâ”€â”€ Home.js        # Landing page
â”‚   â”‚   â”‚   â””â”€â”€ NotFound.js    # 404 page
â”‚   â”‚   â”œâ”€â”€ services/          # API client
â”‚   â”‚   â”‚   â””â”€â”€ api.js         # Backend API communication
â”‚   â”‚   â”œâ”€â”€ styles/            # CSS files
â”‚   â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â””â”€â”€ App.css
â”‚   â”‚   â”œâ”€â”€ App.js             # Main React component
â”‚   â”‚   â””â”€â”€ index.js           # Entry point
â”‚   â””â”€â”€ package.json           # Frontend dependencies
â”‚
â”œâ”€â”€ README.md                  # Project documentation
â”œâ”€â”€ LICENSE                    # MIT License
â””â”€â”€ .gitignore                 # Git ignore file
```

---

## API Endpoints

### POST /api/upload
Upload a video file for speech analysis.

**Request:**
- `file`: Video file (multipart/form-data)

**Response:**
```json
{
  "success": true,
  "video_id": "unique-id",
  "emotion_segments": [...],
  "transcription_data": [...],
  "gemini_analysis": {...},
  "emotion_metrics": {...},
  "speech_clarity": {...},
  "wps_data": [...],
  "duration": 120.5
}
```

### POST /api/chat
Interact with the AI speech coach.

**Request:**
```json
{
  "message": "How can I speak slower?",
  "emotion_segments": [...]
}
```

**Response:**
```json
{
  "response": "To speak slower, try...",
  "audio_url": "data:audio/wav;base64,..."
}
```

### GET /api/healthcheck
Check API server health.

**Response:**
```json
{
  "status": "ok",
  "services": {
    "gemini": "available",
    "deepgram": "available"
  }
}
```

---

## Features in Detail

### Speech Emotion Recognition
Uses the `r-f/wav2vec-english-speech-emotion-recognition` model from Hugging Face to detect emotions in speech segments including: angry, calm, sad, surprised, happy, neutral, anxious, disappointed, fearful, and excited.

### Deepgram Transcription
Utilizes Deepgram's Nova-2 model with Smart Formatting to provide:
- Accurate speech-to-text transcription
- Automatic punctuation
- Proper capitalization
- Timestamp data for each segment

### Speaking Rate Analysis
Calculates words per second (WPS) for each segment with:
- Optimal range indicators (2.0-3.0 WPS)
- Visual feedback (too fast, too slow, optimal)
- Variation metrics for engagement analysis

### Gemini AI Feedback
Generates comprehensive feedback including:
- Overall speech summary
- Identified strengths
- Areas for improvement
- Personalized coaching tips
- Practice exercises

### AI Speech Coach
Interactive chatbot powered by Gemini that:
- Answers specific questions about your speech
- Provides personalized coaching advice
- References your emotion patterns
- Offers audio feedback via Deepgram TTS

---

## Environment Variables

Create a `.env` file in the `backend` directory:

```env
# Required API Keys
GEMINI_API_KEY=your_gemini_api_key_here
DEEPGRAM_API_KEY=your_deepgram_api_key_here

# Optional Configuration
FFMPEG_PATH=ffmpeg
FLASK_ENV=development
FLASK_DEBUG=True
```

---

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## Acknowledgments

- Special thanks to the open-source community for providing the tools and libraries that make this project possible
- Deepgram for their excellent Speech-to-Text and Text-to-Speech APIs
- Google for the Gemini AI API
- Hugging Face for pre-trained speech emotion recognition models
- Inspired by the need for accessible speech coaching tools for everyone

---

## Troubleshooting

### FFmpeg not found
Make sure FFmpeg is installed and in your system PATH:
- **macOS**: `brew install ffmpeg`
- **Ubuntu/Debian**: `sudo apt-get install ffmpeg`
- **Windows**: Download from https://ffmpeg.org/ and add to PATH

### API Key Issues
- Ensure your API keys are properly set in the `.env` file
- Check that the `.env` file is in the `backend` directory
- Restart the Flask server after updating `.env`

### Module Import Errors
- Make sure you've activated the virtual environment
- Reinstall dependencies: `pip install -r requirements.txt`
- Check Python version (3.8+ required)

---

## Future Enhancements

- ğŸ“¹ Real-time speech analysis during recording
- ğŸ¯ Custom coaching goals and progress tracking
- ğŸ“± Mobile app development
- ğŸŒ Multi-language support
- ğŸ“Š Advanced analytics dashboard
- ğŸ‘¥ Peer comparison features

---

For questions, issues, or feature requests, please open an issue on GitHub.
